{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Whisper transcriptions: pre- & post-processing techniques\n",
    "\n",
    "This notebook offers a guide to improve the Whisper's transcriptions. We'll streamline your audio data via trimming and segmentation, enhancing Whisper's transcription quality. After transcriptions, we'll refine the output by adding punctuation, adjusting product terminology (e.g., 'five two nine' to '529'), and mitigating Unicode issues. These strategies will help improve the clarity of your transcriptions, but remember, customization based on your unique use-case may be beneficial.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started let's import a few different libraries:\n",
    "\n",
    "- [PyDub](http://pydub.com/) is a simple and easy-to-use Python library for audio processing tasks such as slicing, concatenating, and exporting audio files.\n",
    "\n",
    "- The `Audio` class from the `IPython.display` module allows you to create an audio control that can play sound in Jupyter notebooks, providing a straightforward way to play audio data directly in your notebook.\n",
    "\n",
    "- For our audio file, we'll use a fictional earnings call written by ChatGPT and read aloud by the author.This audio file is relatively short, but hopefully provides you with an illustrative idea of how these pre and post processing steps can be applied to any audio file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiaph/nghiaph_workspace_115/thesis/.venv/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import urllib\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/EarningsCall.wav', <http.client.HTTPMessage at 0x7fdce1aa9700>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set download paths\n",
    "EarningsCall_remote_filepath = \"https://cdn.openai.com/API/examples/data/EarningsCall.wav\"\n",
    "\n",
    "# set local save locations\n",
    "EarningsCall_filepath = \"data/EarningsCall.wav\"\n",
    "\n",
    "# download example audio files and save locally\n",
    "urllib.request.urlretrieve(EarningsCall_remote_filepath, EarningsCall_filepath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, files with long silences at the beginning can cause Whisper to transcribe the audio incorrectly. We'll use Pydub to detect and trim the silence. \n",
    "\n",
    "Here, we've set the decibel threshold of 20. You can change this if you would like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect leading silence\n",
    "# Returns the number of milliseconds until the first sound (chunk averaging more than X decibels)\n",
    "def milliseconds_until_sound(sound, silence_threshold_in_decibels=-20.0, chunk_size=10):\n",
    "    trim_ms = 0  # ms\n",
    "\n",
    "    assert chunk_size > 0  # to avoid infinite loop\n",
    "    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold_in_decibels and trim_ms < len(sound):\n",
    "        trim_ms += chunk_size\n",
    "\n",
    "    return trim_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_start(filepath):\n",
    "    path = Path(filepath)\n",
    "    directory = path.parent\n",
    "    filename = path.name\n",
    "    audio = AudioSegment.from_file(filepath, format=\"wav\")\n",
    "    start_trim = milliseconds_until_sound(audio)\n",
    "    trimmed = audio[start_trim:]\n",
    "    new_filename = directory / f\"trimmed_{filename}\"\n",
    "    trimmed.export(new_filename, format=\"wav\")\n",
    "    return trimmed, new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(file,output_dir):\n",
    "    audio_path = os.path.join(output_dir, file)\n",
    "    with open(audio_path, 'rb') as audio_data:\n",
    "        transcription = openai.Audio.transcribe(\"whisper-1\", audio_data)\n",
    "        return transcription['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, we've seen unicode character injection in transcripts, removing any non-ASCII characters should help mitigate this issue.\n",
    "\n",
    "Keep in mind you should not use this function if you are transcribing in Greek, Cyrillic, Arabic, Chinese, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove non-ascii characters\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i)<128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will add formatting and punctuation to our transcript. Whisper generates a transcript with punctuation but without formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to add punctuation\n",
    "def punctuation_assistant(ascii_transcript):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant that adds punctuation to text. Preserve the original words and only insert necessary punctuation such as periods, commas, capialization, symbols like dollar sings or percentage signs, and formatting. Use only the context provided. If there is no context provided say, 'No context provided'\\n\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": ascii_transcript  \n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our audio file is a recording from a fake earnings call that includes a lot of financial products. This function can help ensure that if Whisper transcribes these financial product names incorrectly, that they can be corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fix product mispellings\n",
    "def product_assistant(ascii_transcript):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an intelligent assistant specializing in financial products; your task is to process transcripts of earnings calls, ensuring that all references to financial products and common financial terms are in the correct format. For each financial product or common term that is typically abbreviated as an acronym, the full term should be spelled out followed by the acronym in parentheses. For example, '401k' should be transformed to '401(k) retirement savings plan', 'HSA' should be transformed to 'Health Savings Account (HSA)', 'ROA' should be transformed to 'Return on Assets (ROA)', 'VaR' should be transformed to 'Value at Risk (VaR)', and 'PB' should be transformed to 'Price to Book (PB) ratio'. Similarly, transform spoken numbers representing financial products into their numeric representations, followed by the full name of the product in parentheses. For instance, 'five two nine' to '529 (Education Savings Plan)' and 'four zero one k' to '401(k) (Retirement Savings Plan)'. However, be aware that some acronyms can have different meanings based on the context (e.g., 'LTV' can stand for 'Loan to Value' or 'Lifetime Value'). You will need to discern from the context which term is being referred to and apply the appropriate transformation. In cases where numerical figures or metrics are spelled out but do not represent specific financial products (like 'twenty three percent'), these should be left as is. Your role is to analyze and adjust financial product terminology in the text. Once you've done that, produce the adjusted transcript and a list of the words you've changed\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": ascii_transcript  \n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a new file with 'trimmed' appended to the original file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the start of the original audio file\n",
    "trimmed_audio = trim_start(EarningsCall_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_audio, trimmed_filename = trim_start(EarningsCall_filepath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fake earnings report audio file is fairly short in length, so we'll adjust the segments accordingly. Keep in mind you can adjust the segment length as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment audio\n",
    "trimmed_audio = AudioSegment.from_wav(trimmed_filename)  # Load the trimmed audio file\n",
    "\n",
    "one_minute = 1 * 60 * 1000  # Duration for each segment (in milliseconds)\n",
    "\n",
    "start_time = 0  # Start time for the first segment\n",
    "\n",
    "i = 0  # Index for naming the segmented files\n",
    "\n",
    "output_dir_trimmed = \"TrimmedEarningsDirectory\"  # Output directory for the segmented files\n",
    "\n",
    "if not os.path.isdir(output_dir_trimmed):  # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir_trimmed)\n",
    "\n",
    "while start_time < len(trimmed_audio):  # Loop over the trimmed audio file\n",
    "    segment = trimmed_audio[start_time:start_time + one_minute]  # Extract a segment\n",
    "    segment.export(os.path.join(output_dir_trimmed, f\"trimmed_{i:02d}.wav\"), format=\"wav\")  # Save the segment\n",
    "    start_time += one_minute  # Update the start time for the next segment\n",
    "    i += 1  # Increment the index for naming the next file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of trimmed and segmented audio files and sort them numerically\n",
    "audio_files = sorted(\n",
    "    (f for f in os.listdir(output_dir_trimmed) if f.endswith(\".wav\")),\n",
    "    key=lambda f: int(''.join(filter(str.isdigit, f)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use a loop to apply the transcribe function to all audio files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m transcriptions \u001b[39m=\u001b[39m [transcribe_audio(file, output_dir_trimmed) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m audio_files]\n",
      "\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use a loop to apply the transcribe function to all audio files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m transcriptions \u001b[39m=\u001b[39m [transcribe_audio(file, output_dir_trimmed) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m audio_files]\n",
      "\u001b[1;32m/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m audio_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, file)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(audio_path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m audio_data:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     transcription \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mAudio\u001b[39m.\u001b[39;49mtranscribe(\u001b[39m\"\u001b[39;49m\u001b[39mwhisper-1\u001b[39;49m\u001b[39m\"\u001b[39;49m, audio_data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.166.128.115/home/nghiaph/nghiaph_workspace_115/thesis/Experiment_ASR/Whisper_processing_guide.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m transcription[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/nghiaph_workspace_115/thesis/.venv/lib/python3.8/site-packages/openai/api_resources/audio.py:59\u001b[0m, in \u001b[0;36mAudio.transcribe\u001b[0;34m(cls, model, file, api_key, api_base, api_type, api_version, organization, deployment_id, **params)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranscribe\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m     58\u001b[0m ):\n\u001b[0;32m---> 59\u001b[0m     requestor, files, data \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_request(\n\u001b[1;32m     60\u001b[0m         file\u001b[39m=\u001b[39;49mfile,\n\u001b[1;32m     61\u001b[0m         filename\u001b[39m=\u001b[39;49mfile\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m     62\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     63\u001b[0m         api_key\u001b[39m=\u001b[39;49mapi_key,\n\u001b[1;32m     64\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m     65\u001b[0m         api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[1;32m     66\u001b[0m         api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[1;32m     67\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m     68\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     api_type, api_version \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_api_type_and_version(api_type, api_version)\n\u001b[1;32m     71\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_url(\u001b[39m\"\u001b[39m\u001b[39mtranscriptions\u001b[39m\u001b[39m\"\u001b[39m, deployment_id\u001b[39m=\u001b[39mdeployment_id, api_type\u001b[39m=\u001b[39mapi_type, api_version\u001b[39m=\u001b[39mapi_version)\n",
      "File \u001b[0;32m~/nghiaph_workspace_115/thesis/.venv/lib/python3.8/site-packages/openai/api_resources/audio.py:30\u001b[0m, in \u001b[0;36mAudio._prepare_request\u001b[0;34m(cls, file, filename, model, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_request\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m     29\u001b[0m ):\n\u001b[0;32m---> 30\u001b[0m     requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[1;32m     31\u001b[0m         api_key,\n\u001b[1;32m     32\u001b[0m         api_base\u001b[39m=\u001b[39;49mapi_base \u001b[39mor\u001b[39;49;00m openai\u001b[39m.\u001b[39;49mapi_base,\n\u001b[1;32m     33\u001b[0m         api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[1;32m     34\u001b[0m         api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[1;32m     35\u001b[0m         organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     files: List[Any] \u001b[39m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m     40\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m     41\u001b[0m     }\n",
      "File \u001b[0;32m~/nghiaph_workspace_115/thesis/.venv/lib/python3.8/site-packages/openai/api_requestor.py:139\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[0;32m~/nghiaph_workspace_115/thesis/.venv/lib/python3.8/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "# Use a loop to apply the transcribe function to all audio files\n",
    "transcriptions = [transcribe_audio(file, output_dir_trimmed) for file in audio_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the transcriptions\n",
    "full_transcript = ' '.join(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good afternoon, everyone. And welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of 125 million, a 25% increase year over year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA has surged to 37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to 16 million, which is a noteworthy increase from 10 million in Q2 2022. Our total addressable market has grown substantially thanks to the expansion of our high yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized. debt obligations, and residential mortgage-backed securities. We've also invested $25 million in AAA rated corporate bonds, enhancing our risk adjusted returns. As for our balance sheet, total assets reached $1.5 billion with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition cost dropping by 15% and lifetime value growing by 25%. Our LTVCAC ratio is at an impressive 3.5%. In terms of risk management, we have a value-at-risk model in place with a 99%... confidence level indicating that our maximum loss will not exceed 5 million in the next trading day. We've adopted a conservative approach to managing our leverage and have a healthy tier one capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around 135 million and 8% quarter over quarter growth driven primarily by our cutting edge blockchain solutions and AI driven predictive analytics. We're also excited about the upcoming IPO of our FinTech subsidiary, Pay Plus, which we expect to raise 200 million. Significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us and we look forward to an even more successful Q3. Thank you so much.\n"
     ]
    }
   ],
   "source": [
    "print(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-ascii characters from the transcript\n",
    "ascii_transcript = remove_non_ascii(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good afternoon, everyone. And welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of 125 million, a 25% increase year over year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA has surged to 37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to 16 million, which is a noteworthy increase from 10 million in Q2 2022. Our total addressable market has grown substantially thanks to the expansion of our high yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized. debt obligations, and residential mortgage-backed securities. We've also invested $25 million in AAA rated corporate bonds, enhancing our risk adjusted returns. As for our balance sheet, total assets reached $1.5 billion with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition cost dropping by 15% and lifetime value growing by 25%. Our LTVCAC ratio is at an impressive 3.5%. In terms of risk management, we have a value-at-risk model in place with a 99%... confidence level indicating that our maximum loss will not exceed 5 million in the next trading day. We've adopted a conservative approach to managing our leverage and have a healthy tier one capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around 135 million and 8% quarter over quarter growth driven primarily by our cutting edge blockchain solutions and AI driven predictive analytics. We're also excited about the upcoming IPO of our FinTech subsidiary, Pay Plus, which we expect to raise 200 million. Significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us and we look forward to an even more successful Q3. Thank you so much.\n"
     ]
    }
   ],
   "source": [
    "print(ascii_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use punctuation assistant function\n",
    "response = punctuation_assistant(ascii_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the punctuated transcript from the model's response\n",
    "punctuated_transcript = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good afternoon, everyone. And welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar Q2 with a revenue of $125 million, a 25% increase year over year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our EBITDA has surged to $37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to $16 million, which is a noteworthy increase from $10 million in Q2 2022. Our total addressable market has grown substantially thanks to the expansion of our high yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in collateralized debt obligations, and residential mortgage-backed securities. We've also invested $25 million in AAA rated corporate bonds, enhancing our risk-adjusted returns. As for our balance sheet, total assets reached $1.5 billion with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with customer acquisition cost dropping by 15% and lifetime value growing by 25%. Our LTVCAC ratio is at an impressive 3.5%. In terms of risk management, we have a value-at-risk model in place with a 99% confidence level indicating that our maximum loss will not exceed $5 million in the next trading day. We've adopted a conservative approach to managing our leverage and have a healthy tier one capital ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around $135 million and 8% quarter over quarter growth driven primarily by our cutting-edge blockchain solutions and AI-driven predictive analytics. We're also excited about the upcoming IPO of our FinTech subsidiary, Pay Plus, which we expect to raise $200 million, significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us and we look forward to an even more successful Q3. Thank you so much.\n"
     ]
    }
   ],
   "source": [
    "print(punctuated_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use product assistant function\n",
    "response = product_assistant(punctuated_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final transcript from the model's response\n",
    "final_transcript = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good afternoon, everyone. And welcome to FinTech Plus Sync's second quarter 2023 earnings call. I'm John Doe, CEO of FinTech Plus. We've had a stellar second quarter (Q2) with a revenue of $125 million, a 25% increase year over year. Our gross profit margin stands at a solid 58%, due in part to cost efficiencies gained from our scalable business model. Our Earnings Before Interest, Taxes, Depreciation, and Amortization (EBITDA) has surged to $37.5 million, translating to a remarkable 30% EBITDA margin. Our net income for the quarter rose to $16 million, which is a noteworthy increase from $10 million in Q2 2022. Our total addressable market has grown substantially thanks to the expansion of our high yield savings product line and the new RoboAdvisor platform. We've been diversifying our asset-backed securities portfolio, investing heavily in Collateralized Debt Obligations (CDOs), and Residential Mortgage-Backed Securities (RMBS). We've also invested $25 million in AAA rated corporate bonds, enhancing our risk-adjusted returns. As for our balance sheet, total assets reached $1.5 billion with total liabilities at $900 million, leaving us with a solid equity base of $600 million. Our debt-to-equity ratio stands at 1.5, a healthy figure considering our expansionary phase. We continue to see substantial organic user growth, with Customer Acquisition Cost (CAC) dropping by 15% and Lifetime Value (LTV) growing by 25%. Our LTV to CAC (LTVCAC) ratio is at an impressive 3.5%. In terms of risk management, we have a Value at Risk (VaR) model in place with a 99% confidence level indicating that our maximum loss will not exceed $5 million in the next trading day. We've adopted a conservative approach to managing our leverage and have a healthy Tier 1 Capital Ratio of 12.5%. Our forecast for the coming quarter is positive. We expect revenue to be around $135 million and 8% quarter over quarter growth driven primarily by our cutting-edge blockchain solutions and AI-driven predictive analytics. We're also excited about the upcoming Initial Public Offering (IPO) of our FinTech subsidiary, Pay Plus, which we expect to raise $200 million, significantly bolstering our liquidity and paving the way for aggressive growth strategies. We thank our shareholders for their continued faith in us and we look forward to an even more successful third quarter (Q3). Thank you so much.\n",
      "\n",
      "Words changed: \n",
      "Q2 to second quarter (Q2)\n",
      "EBITDA to Earnings Before Interest, Taxes, Depreciation, and Amortization (EBITDA)\n",
      "CDOs to Collateralized Debt Obligations (CDOs)\n",
      "RMBS to Residential Mortgage-Backed Securities (RMBS)\n",
      "CAC to Customer Acquisition Cost (CAC)\n",
      "LTV to Lifetime Value (LTV)\n",
      "LTVCAC to LTV to CAC (LTVCAC)\n",
      "VaR to Value at Risk (VaR)\n",
      "IPO to Initial Public Offering (IPO)\n",
      "Q3 to third quarter (Q3)\n"
     ]
    }
   ],
   "source": [
    "print(final_transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
